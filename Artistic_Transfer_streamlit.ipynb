{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insaiyancvk/Artistic-Transfer/blob/main/Artistic_Transfer_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdEmBazYY2_z"
      },
      "source": [
        "Note: connect to a GPU runtime, else it wouldn't work as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRHpHwwjRga9"
      },
      "source": [
        "# Setup \n",
        "&emsp;~ 8-20 mins"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title connect google drive\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "8gnMBo7hVtoD",
        "outputId": "24f55bd7-86cf-4830-8598-2d63fd9ca2cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "VImiyFusNnbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78f379e-dc3d-4f23-802e-8c225207d06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5188  100  5188    0     0  57644      0 --:--:-- --:--:-- --:--:-- 57644\n"
          ]
        }
      ],
      "source": [
        "#@title Stramlit Setup\n",
        "!pip install streamlit streamlit-image-select -q\n",
        "# !npm install localtunnel --quiet --loglevel=error\n",
        "!curl -sS -O https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken 2J28tpko02mtYUPnZiwzqrLmIwI_5WeUWyik4fHmXnqUe2UQ1\n",
        "!pip install -q pyngrok\n",
        "!curl -o /usr/local/lib/python3.8/dist-packages/google/protobuf/internal/builder.py https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python/google/protobuf/internal/builder.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR5oOlFUjv43",
        "outputId": "af550523-dd4d-409a-d5a2-981c5ac17392"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  \u001b[01;34m.config\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;32mngrok\u001b[0m*  ngrok-stable-linux-amd64.zip  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gFlKG8ks12XM",
        "outputId": "8216d1cb-6f47-4222-800f-c8fb0761e905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.9/524.9 KB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Stable Diffusion Setup\n",
        "!pip -q install diffusers accelerate transformers scipy ftfy \"ipywidgets>=7,<8\"\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n",
        "!pip -q install streamlit streamlit-image-select\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"utils\"):\n",
        "  os.mkdir(\"utils\")\n",
        "\n",
        "if not os.path.exists(\"utils/robotoblack.ttf\"):\n",
        "  !curl -sS -o utils/robotoblack.ttf https://raw.githubusercontent.com/google/fonts/main/apache/roboto/static/Roboto-Black.ttf\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iRbc4q9wQ-Xa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7936b70-3085-4f61-cc36-0aa2ee6810a3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train2014.zip       100%[===================>]  12.58G  56.2MB/s    in 3m 59s  \n",
            "/content/train\n",
            "/content\n",
            "vgg16-397923af.pth  100%[===================>] 527.79M   205MB/s    in 2.6s    \n"
          ]
        }
      ],
      "source": [
        "#@title Transformer network training utils setup\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/gen_imgs\"):\n",
        "  os.mkdir(\"/content/drive/MyDrive/gen_imgs\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# download and unzip dataset o train\n",
        "!wget -q http://images.cocodataset.org/zips/train2014.zip --show-progress\n",
        "!mkdir train\n",
        "%cd /content/train\n",
        "!unzip -qq /content/train2014.zip\n",
        "%cd ..\n",
        "\n",
        "!wget -q https://download.pytorch.org/models/vgg16-397923af.pth --show-progress\n",
        "!mv vgg16-397923af.pth vgg16.pth\n",
        "!rm train2014.zip\n",
        "SAVE_MODEL_PATH = \"/content/models/\"\n",
        "\n",
        "if not os.path.exists(SAVE_MODEL_PATH):\n",
        "  !mkdir $SAVE_MODEL_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ywAaN8p7RCyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a276804c-afc4-4f70-aba6-30908bf3a392",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/utils\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#@title Segmentation related setup\n",
        "# Download helper scripts to generate mask data\n",
        "\n",
        "if not os.path.exists(\"utils\"):\n",
        "  %mkdir utils\n",
        "%cd utils\n",
        "\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/_ext.so -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/backbone.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/box_utils.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/config.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/dcn_v2.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/output_utils.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/timer.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/yolact.py -q\n",
        "\n",
        "%cd ..\n",
        "!gdown -q \"1vQJnSgZkpsDxsXHf_FKMPL3k_-CYQZKj&confirm=t\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMkQbrriRj2t"
      },
      "source": [
        "# The webapp script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ba-bCiSX5Fd",
        "outputId": "413ef3db-d679-4bce-bb4a-c05546b38d12",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dependencies.py\n"
          ]
        }
      ],
      "source": [
        "#@title Dependencies\n",
        "%%writefile dependencies.py\n",
        "from PIL import Image\n",
        "import os, cv2, torch, json\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, vgg_path=\"vgg16.pth\"):\n",
        "        super(VGG16, self).__init__()\n",
        "        # Load VGG Skeleton, Pretrained Weights\n",
        "        vgg16_features = models.vgg16(pretrained=False)\n",
        "        vgg16_features.load_state_dict(torch.load(vgg_path), strict=False)\n",
        "        self.features = vgg16_features.features\n",
        "\n",
        "        # Turn-off Gradient History\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers = {'3': 'relu1_2', '8': 'relu2_2', '15': 'relu3_3', '22': 'relu4_3'}\n",
        "        features = {}\n",
        "        for name, layer in self.features._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in layers:\n",
        "                features[layers[name]] = x\n",
        "                if (name=='22'):\n",
        "                    break\n",
        "\n",
        "        return features\n",
        "\n",
        "class TransformerNetworkNN(nn.Module):\n",
        "    \"\"\"Feedforward Transformation Network without Tanh\n",
        "    reference: https://arxiv.org/abs/1603.08155 \n",
        "    exact architecture: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerNetworkNN, self).__init__()\n",
        "        self.ConvBlock = nn.Sequential(\n",
        "            ConvLayer(3, 32, 9, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(64, 128, 3, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.ResidualBlock = nn.Sequential(\n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3)\n",
        "        )\n",
        "        self.DeconvBlock = nn.Sequential(\n",
        "            DeconvLayer(128, 64, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            DeconvLayer(64, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 3, 9, 1, norm=\"None\")\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvBlock(x)\n",
        "        x = self.ResidualBlock(x)\n",
        "        out = self.DeconvBlock(x)\n",
        "        return out\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        # Padding Layers\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm==\"instance\"):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm==\"batch\"):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        if (self.norm_type==\"None\"):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "class ResidualLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Residual Learning for Image Recognition\n",
        "\n",
        "    https://arxiv.org/abs/1512.03385\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=128, kernel_size=3):\n",
        "        super(ResidualLayer, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x                     # preserve residual\n",
        "        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n",
        "        out = self.conv2(out)            # 2nd conv layer\n",
        "        out = out + identity             # add residual\n",
        "        return out\n",
        "\n",
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "\n",
        "        # Transposed Convolution \n",
        "        padding_size = kernel_size // 2\n",
        "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm==\"instance\"):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm==\"batch\"):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_transpose(x)\n",
        "        if (self.norm_type==\"None\"):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "# Gram Matrix\n",
        "def gram(tensor):\n",
        "    B, C, H, W = tensor.shape\n",
        "    x = tensor.view(B, C, H*W)\n",
        "    x_t = x.transpose(1, 2)\n",
        "    return  torch.bmm(x, x_t) / (C*H*W)\n",
        "\n",
        "# Load image file\n",
        "def load_image(path):\n",
        "    # Images loaded as BGR\n",
        "    img = cv2.imread(path)\n",
        "    return img\n",
        "\n",
        "def saveimg(img, image_path):\n",
        "    img = img.clip(0, 255)\n",
        "    cv2.imwrite(image_path, img)\n",
        "\n",
        "# Preprocessing ~ Image to Tensor\n",
        "def itot(img, max_size=None):\n",
        "    # Rescale the image\n",
        "    if (max_size==None):\n",
        "        itot_t = transforms.Compose([\n",
        "            #transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])    \n",
        "    else:\n",
        "        H, W, C = img.shape\n",
        "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
        "        itot_t = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])\n",
        "\n",
        "    # Convert image to tensor\n",
        "    tensor = itot_t(img)\n",
        "\n",
        "    # Add the batch_size dimension\n",
        "    tensor = tensor.unsqueeze(dim=0)\n",
        "    return tensor\n",
        "\n",
        "# Preprocessing ~ Tensor to Image\n",
        "def ttoi(tensor):\n",
        "\n",
        "    # Remove the batch_size dimension\n",
        "    tensor = tensor.squeeze()\n",
        "    #img = ttoi_t(tensor)\n",
        "    img = tensor.cpu().numpy()\n",
        "    \n",
        "    # Transpose from [C, H, W] -> [H, W, C]\n",
        "    img = img.transpose(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "TRAIN_IMAGE_SIZE = 512\n",
        "DATASET_PATH = \"/content/train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"/content/style.jpg\"\n",
        "BATCH_SIZE = 4\n",
        "CONTENT_WEIGHT = 17\n",
        "STYLE_WEIGHT = 50\n",
        "TV_WEIGHT = 1e-6\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"/content/models/\"\n",
        "SAVE_IMAGE_PATH = \"/content/images/\"\n",
        "SAVE_MODEL_EVERY = 200 # 800 Images with batch size 4\n",
        "SEED = 68\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Dataset and Dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(TRAIN_IMAGE_SIZE),\n",
        "    transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.mul(255))\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Get Style Features\n",
        "imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float16).reshape(1,3,1,1).to(device)\n",
        "imagenet_mean = torch.tensor([103.939, 116.779, 123.68], dtype=torch.float16).reshape(1,3,1,1).to(device)\n",
        "\n",
        "## ----------------------------------------------     Segmentation stuff     ---------------------------------------------------------- ##\n",
        "from utils.yolact import Yolact\n",
        "from utils.output_utils import postprocess\n",
        "from utils.config import cfg, set_cfg\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def calc_size_preserve_ar(img_w, img_h, max_size):\n",
        "    ratio = sqrt(img_w / img_h)\n",
        "    w = max_size * ratio\n",
        "    h = max_size / ratio\n",
        "    return int(w), int(h)\n",
        "\n",
        "class FastBaseTransform(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.mean = torch.Tensor((103.94, 116.78, 123.68)).float().cuda()[None, :, None, None]\n",
        "          self.std  = torch.Tensor( (57.38, 57.12, 58.40) ).float().cuda()[None, :, None, None]\n",
        "    \n",
        "        else:\n",
        "          self.mean = torch.Tensor((103.94, 116.78, 123.68)).float()[None, :, None, None]\n",
        "          self.std  = torch.Tensor( (57.38, 57.12, 58.40) ).float()[None, :, None, None]\n",
        "    \n",
        "        self.transform = cfg.backbone.transform\n",
        "    \n",
        "    def forward(self, img):\n",
        "        self.mean = self.mean.to(img.device)\n",
        "        self.std  = self.std.to(img.device)\n",
        "        \n",
        "        # img assumed to be a pytorch BGR image with channel order [n, h, w, c]\n",
        "        if cfg.preserve_aspect_ratio:\n",
        "            _, h, w, _ = img.size()\n",
        "            img_size = calc_size_preserve_ar(w, h, cfg.max_size)\n",
        "            img_size = (img_size[1], img_size[0]) # Pytorch needs h, w\n",
        "        else:\n",
        "            img_size = (cfg.max_size, cfg.max_size)\n",
        "\n",
        "        img = img.permute(0, 3, 1, 2).contiguous()\n",
        "        img = F.interpolate(img, img_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        if self.transform.normalize:\n",
        "            img = (img - self.mean) / self.std\n",
        "        elif self.transform.subtract_means:\n",
        "            img = (img - self.mean)\n",
        "        elif self.transform.to_float:\n",
        "            img = img / 255\n",
        "        \n",
        "        if self.transform.channel_order != 'RGB':\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        img = img[:, (2, 1, 0), :, :].contiguous()\n",
        "\n",
        "        return img\n",
        "\n",
        "def evalimage(net:Yolact, path:str):\n",
        "    if torch.cuda.is_available():\n",
        "      frame = torch.from_numpy(cv2.imread(path)).cuda().float()\n",
        "    else:\n",
        "      frame = torch.from_numpy(cv2.imread(path)).float()\n",
        "    batch = FastBaseTransform()(frame.unsqueeze(0))\n",
        "    preds = net(batch)\n",
        "    h, w, _ = frame.shape\n",
        "    t = postprocess(preds, w, h, score_threshold = 0.15)\n",
        "    idx = t[1].argsort(0, descending=True)[:15]\n",
        "    masks = t[3][idx]\n",
        "    np.save('utils/mask_data', masks.cpu().numpy())\n",
        "\n",
        "def evaluate(net:Yolact, inp):\n",
        "    net.detect.use_fast_nms = True\n",
        "    net.detect.use_cross_class_nms = False\n",
        "    cfg.mask_proto_debug = False\n",
        "    evalimage(net, inp)\n",
        "    return\n",
        "\n",
        "\n",
        "def segments():\n",
        "    global solutions\n",
        "    set_cfg(\"yolact_plus_base_config\")\n",
        "    with torch.no_grad():\n",
        "      if torch.cuda.is_available():\n",
        "          cudnn.fastest = True\n",
        "          torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "      else:\n",
        "          torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "      net = Yolact()\n",
        "      net.load_weights(\"yolact_plus_base_54_800000.pth\")\n",
        "      net.eval()\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          net = net.cuda()\n",
        "\n",
        "      evaluate(net, \"content.jpg\")\n",
        "\n",
        "    mask = np.load('./utils/mask_data.npy')\n",
        "    sizes = {}\n",
        "    for x,i in enumerate(mask):\n",
        "        sizes[x] = len((np.argwhere(i!=0)))\n",
        "    sizes = {k: v for k, v in sorted(sizes.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "    solutions = {}\n",
        "    for x,i in enumerate(list(sizes.keys())[:4]):\n",
        "        solutions[x] = np.argwhere(mask[i] != 0)\n",
        "\n",
        "    img = cv2.imread('content.jpg')[:, :, ::-1]\n",
        "\n",
        "    segments = {}\n",
        "    for i in range(len(solutions)):\n",
        "        segments[i] = np.full((img.shape), 255.)\n",
        "\n",
        "    for x,i in solutions.items():\n",
        "        for j in i:\n",
        "            segments[x][j[0]][j[1]] = img[j[0]][j[1]]\n",
        "\n",
        "    if not os.path.exists('segments'):\n",
        "        os.mkdir('segments')\n",
        "\n",
        "    for i in range(len(segments)):\n",
        "        im = Image.fromarray(segments[i].astype('uint8'))\n",
        "        im.save(f'segments/{i}.jpg')\n",
        "    \n",
        "    return solutions\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjHQkbJ18RbG",
        "outputId": "72eb44ef-a54a-4563-d1f3-93db5de49646",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "#@title Streamlit web app\n",
        "%%writefile app.py\n",
        "import json\n",
        "import streamlit as st\n",
        "from streamlit_image_select import image_select\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from time import sleep\n",
        "from dependencies import \\\n",
        "                  TransformerNetworkNN, \\\n",
        "                  VGG16, \\\n",
        "                  device, \\\n",
        "                  load_image, \\\n",
        "                  itot, \\\n",
        "                  STYLE_IMAGE_PATH, \\\n",
        "                  imagenet_neg_mean, \\\n",
        "                  BATCH_SIZE, \\\n",
        "                  gram, \\\n",
        "                  optim, \\\n",
        "                  ADAM_LR, \\\n",
        "                  train_loader, \\\n",
        "                  CONTENT_WEIGHT, \\\n",
        "                  STYLE_WEIGHT, \\\n",
        "                  SAVE_MODEL_EVERY, \\\n",
        "                  NumpyEncoder, \\\n",
        "                  NUM_EPOCHS, \\\n",
        "                  segments, \\\n",
        "                  ttoi, \\\n",
        "                  saveimg\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch, os, glob, time, cv2, shutil\n",
        "from PIL import Image \n",
        "\n",
        "num_cols = 3\n",
        "num_rows = 3\n",
        "\n",
        "st.title(\"Artistic Transfer\")\n",
        "\n",
        "if 'is_expanded_first' not in st.session_state:\n",
        "    st.session_state['is_expanded_first'] = True\n",
        "first_container = st.expander(\"Generate a Style Image\", expanded=st.session_state['is_expanded_first'])\n",
        "with first_container:\n",
        "  PROMPT = st.text_input(\"\",placeholder=\"Enter a prompt to generate images\")\n",
        "  st.write('Example: \"Japanese abstract art illustration\"')\n",
        "\n",
        "  gen_imgs = []\n",
        "\n",
        "  if not os.path.exists('Generated_images'):\n",
        "    os.mkdir('Generated_images')\n",
        "\n",
        "  if len(PROMPT) != 0:\n",
        "    if not os.path.exists(f\"Generated_images/{PROMPT}\"):\n",
        "      with st.info('Setting up Stable Diffusion Pipeline', icon=\"ℹ️\"):\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "          \"runwayml/stable-diffusion-v1-5\",\n",
        "          revision=\"fp16\",\n",
        "          torch_dtype=torch.float16,\n",
        "          low_cpu_mem_usage = True,\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "      prompt = [PROMPT] * num_cols # input from user\n",
        "\n",
        "      with st.spinner('Generating images...'):\n",
        "          for _ in range(num_rows):\n",
        "            images = pipe(prompt).images\n",
        "            gen_imgs.extend(images)\n",
        "\n",
        "      os.mkdir(f'Generated_images/{PROMPT}')\n",
        "\n",
        "      for x, i in enumerate(gen_imgs):\n",
        "        i.save(f'Generated_images/{PROMPT}/{x}.jpg')\n",
        "      \n",
        "      if not os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"):\n",
        "        os.mkdir(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\")\n",
        "      \n",
        "      for j,i in enumerate(gen_imgs):\n",
        "        i.save(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/{j}.jpg\")\n",
        "\n",
        "    if len(gen_imgs) == 0:\n",
        "      for f in glob.iglob(f\"Generated_images/{PROMPT}/*\"):\n",
        "          gen_imgs.append(Image.open(f))\n",
        "    all_images = gen_imgs\n",
        "\n",
        "    SELECTED_IMAGE = image_select(\n",
        "        label=\"Select an image\",\n",
        "        images=all_images,\n",
        "        captions=[str(i) for i in range(1,10)],\n",
        "        use_container_width = False\n",
        "    )\n",
        "\n",
        "    if SELECTED_IMAGE:\n",
        "      st.header(\"Selected Image:\")\n",
        "      st.image(SELECTED_IMAGE)\n",
        "    if st.button(\"Proceed further\"):\n",
        "      with st.spinner('Saving the style image'):\n",
        "        SELECTED_IMAGE.save('style.jpg')\n",
        "  sleep(2)\n",
        "  st.session_state['is_expanded_first'] = False\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/train\"):\n",
        "  TransformerNetwork = TransformerNetworkNN().to(device)\n",
        "  st.session_state['is_expanded_sec'] = False\n",
        "  if 'is_expanded_sec' not in st.session_state:\n",
        "    st.session_state['is_expanded_sec'] = True\n",
        "  sec_container = st.expander(\"Transformer network\", expanded=st.session_state['is_expanded_sec'])\n",
        "  with sec_container:\n",
        "    if os.path.exists(f'Generated_images/{PROMPT}'):\n",
        "      try:\n",
        "        del pipe\n",
        "        torch.cuda.empty_cache()\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "    option = st.selectbox(\n",
        "      'Select an option to proceed further',\n",
        "      ('Load a pretrained network', 'Train a transformer network'))\n",
        "\n",
        "    if option == \"Load a pretrained network\":\n",
        "      pretrained_prompt = st.text_input(\"\",placeholder=\"Enter the prompt was previously used for training the network\")\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/style.pth\"):\n",
        "        TRAIN_IMAGE_SIZE = 512\n",
        "        DATASET_PATH = \"/content/train\"\n",
        "        SEED = 68\n",
        "        device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        st.write(\"Loading the pretrained weights\")\n",
        "        TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/style.pth\", map_location=device))\n",
        "    \n",
        "    elif option == \"Train a transformer network\":\n",
        "      if os.path.exists(\"style.jpg\"):\n",
        "        \n",
        "        TRAIN_TIME = st.slider('Select a training time', 30, 120, 5)\n",
        "        st.write(\"Set training time (min). Higher the training time, better the style transfer by the network.\")\n",
        "        \n",
        "        if st.button(\"Start training\"):\n",
        "          st.session_state['is_expanded_first'] = False\n",
        "          if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\"):\n",
        "            with st.spinner(\"Pretrained weights found. Loading it\"):\n",
        "              TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\", map_location=device))\n",
        "          VGG = VGG16('/content/vgg16.pth').to(device)\n",
        "          style_image = load_image(STYLE_IMAGE_PATH)\n",
        "          style_tensor = itot(style_image).to(device)\n",
        "          style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "          B, C, H, W = style_tensor.shape\n",
        "          style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "          style_gram = {}\n",
        "          for key, value in style_features.items():\n",
        "              style_gram[key] = gram(value)\n",
        "\n",
        "          # Optimizer settings\n",
        "          optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
        "          MODEL_NAME = \"style\"\n",
        "          DRIVE_PATH = f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"\n",
        "          batch_count = 1\n",
        "          TRAIN_TIME *= 60\n",
        "          start_time = time.time()\n",
        "          progress_bar = st.progress(0)\n",
        "          latest_iteration = st.empty()\n",
        "\n",
        "          for content_batch, _ in train_loader:\n",
        "            curr_batch_size = content_batch.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = TransformerNetwork(content_batch)\n",
        "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # Content Loss\n",
        "            MSELoss = nn.MSELoss().to(device)\n",
        "            content_loss = CONTENT_WEIGHT * MSELoss(content_features['relu2_2'], generated_features['relu2_2'])\n",
        "\n",
        "            # Style Loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = content_loss + style_loss\n",
        "\n",
        "            # Backprop and Weight Update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                torch.save(TransformerNetwork.state_dict(), f\"{DRIVE_PATH}/{MODEL_NAME}.pth\")\n",
        "            batch_count+=1\n",
        "            time_passed = time.time()-start_time\n",
        "            progress_time = time_passed/TRAIN_TIME\n",
        "            if progress_time < 1:\n",
        "              latest_iteration.text(f\"{(time_passed/60):.1f}/{TRAIN_TIME//60} mins\")\n",
        "              progress_bar.progress(progress_time)\n",
        "            else:\n",
        "              progress_bar.progress(1)\n",
        "\n",
        "            if (time.time()-start_time) > TRAIN_TIME:\n",
        "              latest_iteration.success('Training completed!', icon=\"✅\")\n",
        "              break\n",
        "\n",
        "      else:\n",
        "        st.write(\"Generate and choose a style image\")\n",
        "\n",
        "INDEX = None\n",
        "solutions = {}\n",
        "\n",
        "with st.expander(\"Segment an Image\"):\n",
        "\n",
        "  segimgs = []\n",
        "  image_file = st.file_uploader(\"Upload an image to segment\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "  \n",
        "  if image_file is not None:\n",
        "    with open(\"content.jpg\",\"wb\") as f: \n",
        "      f.write(image_file.getbuffer())\n",
        "\n",
        "  if image_file is None:\n",
        "    if os.path.exists('content.jpg'):\n",
        "      os.remove('content.jpg')\n",
        "    if os.path.exists('utils/segment.jpg'):\n",
        "      os.remove(\"utils/segment.jpg\")\n",
        "      solutions = {}\n",
        "    if os.path.exists('segments'):\n",
        "      shutil.rmtree('segments')\n",
        "\n",
        "  if os.path.exists(\"content.jpg\") and (not os.path.exists(\"segments\")):\n",
        "    if st.button(\"Generate segments\"):\n",
        "      with st.spinner('Generating segments...'):\n",
        "        solutions = segments()\n",
        "        with open('utils/solutions.json', 'w') as f:\n",
        "          json.dump(json.dumps(solutions, cls=NumpyEncoder), f)\n",
        "\n",
        "  if os.path.exists(\"segments\") and len(segimgs)==0:\n",
        "    segimgs = [Image.open(\"segments/\"+f) for f in sorted(os.listdir(\"segments\"))]\n",
        "\n",
        "  if len(segimgs)>0:\n",
        "    INDEX = image_select(\n",
        "        label=\"Select a segment\",\n",
        "        images=segimgs,\n",
        "        return_value=\"index\",\n",
        "        use_container_width = False\n",
        "    )\n",
        "\n",
        "    if (INDEX or INDEX==0) and segimgs[INDEX]:\n",
        "      st.header(\"Selected Segment:\")\n",
        "      segimgs[INDEX].save('utils/segment.jpg')\n",
        "      st.image(segimgs[INDEX])\n",
        "\n",
        "with st.expander(\"Style transfer on the segment\"):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  if os.path.exists(\"utils/segment.jpg\") :\n",
        "    if st.button(\"Apply style transfer\"):\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "        content_image = load_image(\"utils/segment.jpg\")\n",
        "        content_tensor = itot(content_image).to(device)\n",
        "        TransformerNetwork = TransformerNetwork.to(device)\n",
        "        generated_tensor = TransformerNetwork(content_tensor)\n",
        "        generated_image = ttoi(generated_tensor.detach())\n",
        "        saveimg(generated_image, \"utils/segment_style.jpg\")\n",
        "\n",
        "      with st.spinner(\"Applying style transfer\"):\n",
        "        nimg = cv2.imread('content.jpg')[:, :, ::-1]\n",
        "        styleimg = cv2.imread('utils/segment_style.jpg')[:,:,::-1]\n",
        "        if os.path.exists('utils/solutions.json'):\n",
        "          with open('utils/solutions.json') as f:\n",
        "            solutions = json.loads(json.load(f))\n",
        "        \n",
        "        for i in solutions[str(INDEX)]:\n",
        "          nimg[i[0]][i[1]] = styleimg[i[0]][i[1]]\n",
        "\n",
        "        img = Image.fromarray(nimg.astype('uint8'))\n",
        "        img.save('Final Transformation.png')\n",
        "  else:\n",
        "    st.write(\"Upload a content image to generate segments\")\n",
        "\n",
        "if os.path.exists('Final Transformation.png'):\n",
        "\n",
        "  if 'final_trans' not in st.session_state:\n",
        "    st.session_state.final_trans = 'noimg'\n",
        "  st.image(\"Final Transformation.png\")\n",
        "  st.session_state.final_trans = 'yeimg'\n",
        "\n",
        "  if st.session_state.final_trans == 'yeimg':\n",
        "    try:\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"):\n",
        "        if st.button(\"Save content, style, transformed images to drive\"):\n",
        "          all_imgs =  [\n",
        "            'Final Transformation.png',\n",
        "            'content.jpg',\n",
        "            'style.jpg'\n",
        "        ]\n",
        "        for i in all_imgs:\n",
        "          os.system(f\"!cp {i} /content/drive/MyDrive/gen_imgs/{PROMPT}\")\n",
        "    except NameError:\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4EK0o6qRnn0"
      },
      "source": [
        "# Start the streamlit web app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!streamlit run --server.port 80 app.py&>logs.txt&\n",
        "from time import sleep\n",
        "sleep(5)\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "207qafokLRzV",
        "outputId": "54ef0f64-007c-4a09-ca3b-ebdd2cdba740"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-01-11T16:35:28+0000 lvl=warn msg=\"failed to start tunnel\" pg=/api/tunnels id=8af2c8ee6f40cee8 err=\"Your account may not run more than 3 tunnels over a single ngrok agent session.\\nThe tunnels already running on this session are:\\ntn_2KBjZtPdstoNLqAYYDxqLcVssq8, tn_2KBiCNI1hyVxe4lMkL65zRb1x1Y, tn_2KBiCTXU0GLjISeeLSqrDWdd8hZ\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokHTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7da6924bc044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'8501'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tunnel with options: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     tunnel = NgrokTunnel(api_request(\"{}/api/tunnels\".format(api_url), method=\"POST\", data=options,\n\u001b[0m\u001b[1;32m    272\u001b[0m                                      timeout=pyngrok_config.request_timeout),\n\u001b[1;32m    273\u001b[0m                          pyngrok_config, api_url)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Response {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         raise PyngrokNgrokHTTPError(\"ngrok client exception, API returned {}: {}\".format(status_code, response_data),\n\u001b[0m\u001b[1;32m    478\u001b[0m                                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                                     status_code, e.msg, e.hdrs, response_data)\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m: ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"Your account may not run more than 3 tunnels over a single ngrok agent session.\\nThe tunnels already running on this session are:\\ntn_2KBjZtPdstoNLqAYYDxqLcVssq8, tn_2KBiCNI1hyVxe4lMkL65zRb1x1Y, tn_2KBiCTXU0GLjISeeLSqrDWdd8hZ\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stop the streamlit webapp"
      ],
      "metadata": {
        "id": "-Xr6Vq9W19Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "ovOfFPAzMvMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PRHpHwwjRga9",
        "d4EK0o6qRnn0",
        "-Xr6Vq9W19Cz"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}