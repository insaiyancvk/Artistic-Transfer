{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insaiyancvk/Artistic-Transfer/blob/main/streamlit%20final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdEmBazYY2_z"
      },
      "source": [
        "Note: connect to a GPU runtime, else it wouldn't work as expected"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup appropriate python version\n",
        "#@markdown Note: The session will restart after this cell is run. This step is necessary to setup colab with appropriate versions of python and other libraries.\n",
        "\n",
        "# Setup appropriate python version\n",
        "\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.8 python3.8-dev python3.8-distutils libpython3.8-dev\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3 get-pip.py --force-reinstall\n",
        "!python3 -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "!ln -s /usr/local/lib/python3.9/dist-packages/google \\\n",
        "       /usr/local/lib/python3.8/dist-packages/google\n",
        "\n",
        "!sed -i \"s/from IPython.utils import traitlets as _traitlets/import traitlets as _traitlets/\" /usr/local/lib/python3.8/dist-packages/google/colab/*.py\n",
        "!sed -i \"s/from IPython.utils import traitlets/import traitlets/\" /usr/local/lib/python3.8/dist-packages/google/colab/*.py\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "\n",
        "from time import sleep\n",
        "sleep(5)\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n_C5cexIG2Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note: Run the other setup cells after the previous cell is run."
      ],
      "metadata": {
        "id": "L8wQAAtVfUjD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRHpHwwjRga9"
      },
      "source": [
        "# Setup \n",
        "&emsp;~ 8-20 mins"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title connect google drive\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8gnMBo7hVtoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49da1db1-3944-430b-cf98-4984a314968f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip uninstall torch -y\n",
        "!pip install -q torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install -q matplotlib opencv-python gdown"
      ],
      "metadata": {
        "id": "o3SsIhxF9Auu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "VImiyFusNnbf"
      },
      "outputs": [],
      "source": [
        "#@title Stramlit Setup\n",
        "!pip install streamlit streamlit-image-select -q\n",
        "# !npm install localtunnel --quiet --loglevel=error\n",
        "import os\n",
        "if not os.path.exists(\"/content/ngrok\"):\n",
        "  !curl -sS -O https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "  !unzip ngrok-stable-linux-amd64.zip\n",
        "  !./ngrok authtoken 2J28tpko02mtYUPnZiwzqrLmIwI_5WeUWyik4fHmXnqUe2UQ1\n",
        "\n",
        "!pip install -q pyngrok==5.2.1\n",
        "if not os.path.exists(\"/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/builder.py\"):\n",
        "  !curl -o /usr/local/lib/python3.8/dist-packages/google/protobuf/internal/builder.py https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python/google/protobuf/internal/builder.py 2>/dev/null\n",
        "\n",
        "if not os.path.exists(\"/usr/local/lib/python3.9/dist-packages/google/protobuf/internal/builder.py\"):\n",
        "  !curl -o /usr/local/lib/python3.9/dist-packages/google/protobuf/internal/builder.py https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python/google/protobuf/internal/builder.py 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "gFlKG8ks12XM"
      },
      "outputs": [],
      "source": [
        "#@title Stable Diffusion Setup\n",
        "!pip -q install diffusers accelerate transformers scipy ftfy \"ipywidgets>=7,<8\" >/dev/null\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n",
        "!pip -q install streamlit streamlit-image-select\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"utils\"):\n",
        "  os.mkdir(\"utils\")\n",
        "\n",
        "if not os.path.exists(\"utils/robotoblack.ttf\"):\n",
        "  !curl -sS -o utils/robotoblack.ttf https://raw.githubusercontent.com/google/fonts/main/apache/roboto/static/Roboto-Black.ttf\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRbc4q9wQ-Xa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Transformer network training utils setup\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/gen_imgs\"):\n",
        "  os.mkdir(\"/content/drive/MyDrive/gen_imgs\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# download and unzip dataset o train\n",
        "if not (os.path.exists(\"/content/train2014.zip\") or os.path.exists(\"/content/train/train2014\")):\n",
        "  !wget -q http://images.cocodataset.org/zips/train2014.zip --show-progress\n",
        "  !mkdir train\n",
        "  %cd /content/train\n",
        "  !unzip -qq /content/train2014.zip\n",
        "  %cd ..\n",
        "  !rm train2014.zip\n",
        "\n",
        "if not os.path.exists('vgg16.pth'):\n",
        "  !wget -q https://download.pytorch.org/models/vgg16-397923af.pth --show-progress\n",
        "  !mv vgg16-397923af.pth vgg16.pth\n",
        "\n",
        "SAVE_MODEL_PATH = \"/content/models/\"\n",
        "\n",
        "if not os.path.exists(SAVE_MODEL_PATH):\n",
        "  !mkdir $SAVE_MODEL_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywAaN8p7RCyy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Segmentation related setup\n",
        "# Download helper scripts to generate mask data\n",
        "import os\n",
        "if not os.path.exists(\"utils\"):\n",
        "  %mkdir utils\n",
        "%cd utils\n",
        "\n",
        "files = {\n",
        "    \"_ext.so\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/_ext.so\",\n",
        "    \"backbone.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/backbone.py\",\n",
        "    \"box_utils.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/box_utils.py\",\n",
        "    \"config.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/config.py\",\n",
        "    \"dcn_v2.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/dcn_v2.py\",\n",
        "    \"output_utils.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/output_utils.py\",\n",
        "    \"timer.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/timer.py\",\n",
        "    \"yolact.py\": \"https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/yolact.py\",\n",
        "}\n",
        "\n",
        "for i,j in files.items():\n",
        "  if not os.path.exists(i):\n",
        "    !wget $j -q\n",
        "\n",
        "%cd ..\n",
        "if not os.path.exists(\"/content/yolact_plus_base_54_800000.pth\"):\n",
        "  !gdown -q \"1vQJnSgZkpsDxsXHf_FKMPL3k_-CYQZKj&confirm=t\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMkQbrriRj2t"
      },
      "source": [
        "# The webapp script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ba-bCiSX5Fd",
        "outputId": "96dc96fe-8470-4d56-f500-0f3f549e1599",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dependencies.py\n"
          ]
        }
      ],
      "source": [
        "#@title Dependencies\n",
        "%%writefile dependencies.py\n",
        "from PIL import Image\n",
        "import os, cv2, torch, json\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, vgg_path=\"vgg16.pth\"):\n",
        "        super(VGG16, self).__init__()\n",
        "        # Load VGG Skeleton, Pretrained Weights\n",
        "        vgg16_features = models.vgg16(pretrained=False)\n",
        "        vgg16_features.load_state_dict(torch.load(vgg_path), strict=False)\n",
        "        self.features = vgg16_features.features\n",
        "\n",
        "        # Turn-off Gradient History\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers = {'3': 'relu1_2', '8': 'relu2_2', '15': 'relu3_3', '22': 'relu4_3'}\n",
        "        features = {}\n",
        "        for name, layer in self.features._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in layers:\n",
        "                features[layers[name]] = x\n",
        "                if (name=='22'):\n",
        "                    break\n",
        "\n",
        "        return features\n",
        "\n",
        "class TransformerNetworkNN(nn.Module):\n",
        "    \"\"\"Feedforward Transformation Network without Tanh\n",
        "    reference: https://arxiv.org/abs/1603.08155 \n",
        "    exact architecture: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerNetworkNN, self).__init__()\n",
        "        self.ConvBlock = nn.Sequential(\n",
        "            ConvLayer(3, 32, 9, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(64, 128, 3, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.ResidualBlock = nn.Sequential(\n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3)\n",
        "        )\n",
        "        self.DeconvBlock = nn.Sequential(\n",
        "            DeconvLayer(128, 64, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            DeconvLayer(64, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 3, 9, 1, norm=\"None\")\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvBlock(x)\n",
        "        x = self.ResidualBlock(x)\n",
        "        out = self.DeconvBlock(x)\n",
        "        return out\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        # Padding Layers\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm==\"instance\"):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm==\"batch\"):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        if (self.norm_type==\"None\"):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "class ResidualLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Residual Learning for Image Recognition\n",
        "\n",
        "    https://arxiv.org/abs/1512.03385\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=128, kernel_size=3):\n",
        "        super(ResidualLayer, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x                     # preserve residual\n",
        "        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n",
        "        out = self.conv2(out)            # 2nd conv layer\n",
        "        out = out + identity             # add residual\n",
        "        return out\n",
        "\n",
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "\n",
        "        # Transposed Convolution \n",
        "        padding_size = kernel_size // 2\n",
        "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm==\"instance\"):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm==\"batch\"):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_transpose(x)\n",
        "        if (self.norm_type==\"None\"):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "# Gram Matrix\n",
        "def gram(tensor):\n",
        "    B, C, H, W = tensor.shape\n",
        "    x = tensor.view(B, C, H*W)\n",
        "    x_t = x.transpose(1, 2)\n",
        "    return  torch.bmm(x, x_t) / (C*H*W)\n",
        "\n",
        "# Load image file\n",
        "def load_image(path):\n",
        "    # Images loaded as BGR\n",
        "    img = cv2.imread(path)\n",
        "    return img\n",
        "\n",
        "def saveimg(img, image_path):\n",
        "    img = img.clip(0, 255)\n",
        "    cv2.imwrite(image_path, img)\n",
        "\n",
        "# Preprocessing ~ Image to Tensor\n",
        "def itot(img, max_size=None):\n",
        "    # Rescale the image\n",
        "    if (max_size==None):\n",
        "        itot_t = transforms.Compose([\n",
        "            #transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])    \n",
        "    else:\n",
        "        H, W, C = img.shape\n",
        "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
        "        itot_t = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])\n",
        "\n",
        "    # Convert image to tensor\n",
        "    tensor = itot_t(img)\n",
        "\n",
        "    # Add the batch_size dimension\n",
        "    tensor = tensor.unsqueeze(dim=0)\n",
        "    return tensor\n",
        "\n",
        "# Preprocessing ~ Tensor to Image\n",
        "def ttoi(tensor):\n",
        "\n",
        "    # Remove the batch_size dimension\n",
        "    tensor = tensor.squeeze()\n",
        "    #img = ttoi_t(tensor)\n",
        "    img = tensor.cpu().numpy()\n",
        "    \n",
        "    # Transpose from [C, H, W] -> [H, W, C]\n",
        "    img = img.transpose(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "TRAIN_IMAGE_SIZE = 512\n",
        "DATASET_PATH = \"/content/train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"/content/style.jpg\"\n",
        "BATCH_SIZE = 4\n",
        "CONTENT_WEIGHT = 17\n",
        "STYLE_WEIGHT = 50\n",
        "TV_WEIGHT = 1e-6\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"/content/models/\"\n",
        "SAVE_IMAGE_PATH = \"/content/images/\"\n",
        "SAVE_MODEL_EVERY = 200 # 800 Images with batch size 4\n",
        "SEED = 68\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Dataset and Dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(TRAIN_IMAGE_SIZE),\n",
        "    transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.mul(255))\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Get Style Features\n",
        "imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float16).reshape(1,3,1,1).to(device)\n",
        "imagenet_mean = torch.tensor([103.939, 116.779, 123.68], dtype=torch.float16).reshape(1,3,1,1).to(device)\n",
        "\n",
        "## ----------------------------------------------     Segmentation stuff     ---------------------------------------------------------- ##\n",
        "from utils.yolact import Yolact\n",
        "from utils.output_utils import postprocess\n",
        "from utils.config import cfg, set_cfg\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def calc_size_preserve_ar(img_w, img_h, max_size):\n",
        "    ratio = sqrt(img_w / img_h)\n",
        "    w = max_size * ratio\n",
        "    h = max_size / ratio\n",
        "    return int(w), int(h)\n",
        "\n",
        "class FastBaseTransform(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.mean = torch.Tensor((103.94, 116.78, 123.68)).float().cuda()[None, :, None, None]\n",
        "          self.std  = torch.Tensor( (57.38, 57.12, 58.40) ).float().cuda()[None, :, None, None]\n",
        "    \n",
        "        else:\n",
        "          self.mean = torch.Tensor((103.94, 116.78, 123.68)).float()[None, :, None, None]\n",
        "          self.std  = torch.Tensor( (57.38, 57.12, 58.40) ).float()[None, :, None, None]\n",
        "    \n",
        "        self.transform = cfg.backbone.transform\n",
        "    \n",
        "    def forward(self, img):\n",
        "        self.mean = self.mean.to(img.device)\n",
        "        self.std  = self.std.to(img.device)\n",
        "        \n",
        "        # img assumed to be a pytorch BGR image with channel order [n, h, w, c]\n",
        "        if cfg.preserve_aspect_ratio:\n",
        "            _, h, w, _ = img.size()\n",
        "            img_size = calc_size_preserve_ar(w, h, cfg.max_size)\n",
        "            img_size = (img_size[1], img_size[0]) # Pytorch needs h, w\n",
        "        else:\n",
        "            img_size = (cfg.max_size, cfg.max_size)\n",
        "\n",
        "        img = img.permute(0, 3, 1, 2).contiguous()\n",
        "        img = F.interpolate(img, img_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        if self.transform.normalize:\n",
        "            img = (img - self.mean) / self.std\n",
        "        elif self.transform.subtract_means:\n",
        "            img = (img - self.mean)\n",
        "        elif self.transform.to_float:\n",
        "            img = img / 255\n",
        "        \n",
        "        if self.transform.channel_order != 'RGB':\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        img = img[:, (2, 1, 0), :, :].contiguous()\n",
        "\n",
        "        return img\n",
        "\n",
        "def evalimage(net:Yolact, path:str):\n",
        "    if torch.cuda.is_available():\n",
        "      frame = torch.from_numpy(cv2.imread(path)).cuda().float()\n",
        "    else:\n",
        "      frame = torch.from_numpy(cv2.imread(path)).float()\n",
        "    batch = FastBaseTransform()(frame.unsqueeze(0))\n",
        "    preds = net(batch)\n",
        "    h, w, _ = frame.shape\n",
        "    t = postprocess(preds, w, h, score_threshold = 0.15)\n",
        "    idx = t[1].argsort(0, descending=True)[:15]\n",
        "    masks = t[3][idx]\n",
        "    np.save('utils/mask_data', masks.cpu().numpy())\n",
        "\n",
        "def evaluate(net:Yolact, inp):\n",
        "    net.detect.use_fast_nms = True\n",
        "    net.detect.use_cross_class_nms = False\n",
        "    cfg.mask_proto_debug = False\n",
        "    evalimage(net, inp)\n",
        "    return\n",
        "\n",
        "\n",
        "def segments():\n",
        "    global solutions\n",
        "    set_cfg(\"yolact_plus_base_config\")\n",
        "    with torch.no_grad():\n",
        "      if torch.cuda.is_available():\n",
        "          cudnn.fastest = True\n",
        "          torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "      else:\n",
        "          torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "      net = Yolact()\n",
        "      net.load_weights(\"yolact_plus_base_54_800000.pth\")\n",
        "      net.eval()\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          net = net.cuda()\n",
        "\n",
        "      evaluate(net, \"content.jpg\")\n",
        "\n",
        "    mask = np.load('./utils/mask_data.npy')\n",
        "    sizes = {}\n",
        "    for x,i in enumerate(mask):\n",
        "        sizes[x] = len((np.argwhere(i!=0)))\n",
        "    sizes = {k: v for k, v in sorted(sizes.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "    solutions = {}\n",
        "    for x,i in enumerate(list(sizes.keys())[:4]):\n",
        "        solutions[x] = np.argwhere(mask[i] != 0)\n",
        "\n",
        "    img = cv2.imread('content.jpg')[:, :, ::-1]\n",
        "\n",
        "    segments = {}\n",
        "    for i in range(len(solutions)):\n",
        "        segments[i] = np.full((img.shape), 255.)\n",
        "\n",
        "    for x,i in solutions.items():\n",
        "        for j in i:\n",
        "            segments[x][j[0]][j[1]] = img[j[0]][j[1]]\n",
        "\n",
        "    if not os.path.exists('segments'):\n",
        "        os.mkdir('segments')\n",
        "\n",
        "    for i in range(len(segments)):\n",
        "        im = Image.fromarray(segments[i].astype('uint8'))\n",
        "        im.save(f'segments/{i}.jpg')\n",
        "    \n",
        "    return solutions\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjHQkbJ18RbG",
        "outputId": "e9e6e447-2676-4021-ca61-22d021671c1c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "#@title Streamlit web app\n",
        "%%writefile app.py\n",
        "import json\n",
        "import streamlit as st\n",
        "from streamlit_image_select import image_select\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from time import sleep\n",
        "from dependencies import \\\n",
        "                  TransformerNetworkNN, \\\n",
        "                  VGG16, \\\n",
        "                  device, \\\n",
        "                  load_image, \\\n",
        "                  itot, \\\n",
        "                  STYLE_IMAGE_PATH, \\\n",
        "                  imagenet_neg_mean, \\\n",
        "                  BATCH_SIZE, \\\n",
        "                  gram, \\\n",
        "                  optim, \\\n",
        "                  nn, \\\n",
        "                  ADAM_LR, \\\n",
        "                  train_loader, \\\n",
        "                  CONTENT_WEIGHT, \\\n",
        "                  STYLE_WEIGHT, \\\n",
        "                  SAVE_MODEL_EVERY, \\\n",
        "                  NumpyEncoder, \\\n",
        "                  NUM_EPOCHS, \\\n",
        "                  segments, \\\n",
        "                  ttoi, \\\n",
        "                  saveimg\n",
        "\n",
        "import torch, os, glob, time, cv2, shutil\n",
        "from PIL import Image\n",
        "\n",
        "num_cols = 3\n",
        "num_rows = 3\n",
        "\n",
        "st.title(\"Artistic Transfer\")\n",
        "\n",
        "if 'is_expanded_first' not in st.session_state:\n",
        "    st.session_state['is_expanded_first'] = True\n",
        "first_container = st.expander(\"Generate a Style Image\", expanded=st.session_state['is_expanded_first'])\n",
        "with first_container:\n",
        "  PROMPT = st.text_input(\"\",placeholder=\"Enter a prompt to generate images\")\n",
        "  st.write('Example: \"Japanese abstract art illustration\"')\n",
        "\n",
        "  gen_imgs = []\n",
        "\n",
        "  if not os.path.exists('Generated_images'):\n",
        "    os.mkdir('Generated_images')\n",
        "\n",
        "  if len(PROMPT) != 0:\n",
        "    if not os.path.exists(f\"Generated_images/{PROMPT}\"):\n",
        "      with st.info('Setting up Stable Diffusion Pipeline', icon=\"ℹ️\"):\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "          \"runwayml/stable-diffusion-v1-5\",\n",
        "          revision=\"fp16\",\n",
        "          torch_dtype=torch.float16,\n",
        "          low_cpu_mem_usage = True,\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "      prompt = [PROMPT] * num_cols # input from user\n",
        "\n",
        "      with st.spinner('Generating images...'):\n",
        "          for _ in range(num_rows):\n",
        "            images = pipe(prompt).images\n",
        "            gen_imgs.extend(images)\n",
        "\n",
        "      os.mkdir(f'Generated_images/{PROMPT}')\n",
        "\n",
        "      for x, i in enumerate(gen_imgs):\n",
        "        i.save(f'Generated_images/{PROMPT}/{x}.jpg')\n",
        "      \n",
        "      if not os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"):\n",
        "        os.mkdir(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\")\n",
        "      \n",
        "      if not os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "        with open(f'/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json','w') as f:\n",
        "          json.dump({}, f)\n",
        "\n",
        "      for j,i in enumerate(gen_imgs):\n",
        "        i.save(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/{j}.jpg\")\n",
        "\n",
        "    if len(gen_imgs) == 0:\n",
        "      for f in glob.iglob(f\"Generated_images/{PROMPT}/*\"):\n",
        "          gen_imgs.append(Image.open(f))\n",
        "    all_images = gen_imgs\n",
        "\n",
        "    SELECTED_IMAGE = image_select(\n",
        "        label=\"Select an image\",\n",
        "        images=all_images,\n",
        "        captions=[str(i) for i in range(1,10)],\n",
        "        use_container_width = False\n",
        "    )\n",
        "\n",
        "    if SELECTED_IMAGE:\n",
        "      st.header(\"Selected Image:\")\n",
        "      st.image(SELECTED_IMAGE)\n",
        "    if st.button(\"Proceed further\"):\n",
        "      with st.spinner('Saving the style image'):\n",
        "        SELECTED_IMAGE.save('style.jpg')\n",
        "        SELECTED_IMAGE.save(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.jpg\")\n",
        "\n",
        "  sleep(2)\n",
        "  st.session_state['is_expanded_first'] = False\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/train\"):\n",
        "  TransformerNetwork = TransformerNetworkNN().to(device)\n",
        "  logs = {}\n",
        "  st.session_state['is_expanded_sec'] = False\n",
        "  if 'is_expanded_sec' not in st.session_state:\n",
        "    st.session_state['is_expanded_sec'] = True\n",
        "  sec_container = st.expander(\"Transformer network\", expanded=st.session_state['is_expanded_sec'])\n",
        "  with sec_container:\n",
        "    if os.path.exists(f'Generated_images/{PROMPT}'):\n",
        "      try:\n",
        "        del pipe\n",
        "        torch.cuda.empty_cache()\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "    not_done = []\n",
        "    for i in os.listdir(f\"/content/drive/MyDrive/gen_imgs/\"):\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{i}/logs.json\"):\n",
        "        with open(f\"/content/drive/MyDrive/gen_imgs/{i}/logs.json\") as f:\n",
        "          temp_logs = eval(f.read())\n",
        "        if ('TRAIN_TIME' in list(temp_logs.keys())) and ('TIME_PASSED' in list(temp_logs.keys())):\n",
        "          if temp_logs['TIME_PASSED'] < temp_logs['TRAIN_TIME']:\n",
        "            not_done.append(i)\n",
        "\n",
        "    options = ('Load a pretrained network', 'Train a transformer network')\n",
        "    if len(not_done)>0:\n",
        "        options += ('Continue with other networks training',)\n",
        "\n",
        "    option = st.selectbox(\n",
        "      'Select an option to proceed further',\n",
        "      options)\n",
        "      \n",
        "    if option == \"Load a pretrained network\":\n",
        "      pretrained_prompt = st.text_input(\"\",placeholder=\"Enter the prompt was previously used for training the network\")\n",
        "      if not os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/logs.json\") and len(pretrained_prompt)>0:\n",
        "        with open(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/logs.json\",'w'):\n",
        "          json.dump({}, f)\n",
        "\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/style.pth\"):\n",
        "        TRAIN_IMAGE_SIZE = 512\n",
        "        DATASET_PATH = \"/content/train\"\n",
        "        SEED = 68\n",
        "        device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        st.write(\"Loading the pretrained weights\")\n",
        "        TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/style.pth\", map_location=device))\n",
        "    \n",
        "    elif option == \"Train a transformer network\":\n",
        "      if os.path.exists(\"style.jpg\"):\n",
        "        if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "            with open(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\") as f:\n",
        "              logs = eval(f.read())\n",
        "\n",
        "        TRAIN_TIME = st.slider(label='Select a training time', min_value=30, max_value=120, step=5)\n",
        "        logs['TRAIN_TIME'] = TRAIN_TIME\n",
        "        st.write(\"Set training time (min). Higher the training time, better the style transfer by the network.\")\n",
        "        \n",
        "        if st.button(\"Start training\"):\n",
        "          st.session_state['is_expanded_first'] = False\n",
        "          if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\"):\n",
        "            with st.spinner(\"Pretrained weights found. Loading it\"):\n",
        "              TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\", map_location=device))\n",
        "\n",
        "          VGG = VGG16('/content/vgg16.pth').to(device)\n",
        "          style_image = load_image(STYLE_IMAGE_PATH)\n",
        "          style_tensor = itot(style_image).to(device)\n",
        "          style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "          B, C, H, W = style_tensor.shape\n",
        "          style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "          style_gram = {}\n",
        "          for key, value in style_features.items():\n",
        "              style_gram[key] = gram(value)\n",
        "\n",
        "          # Optimizer settings\n",
        "          optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
        "          MODEL_NAME = \"style\"\n",
        "          DRIVE_PATH = f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"\n",
        "          batch_count = 1\n",
        "          TRAIN_TIME *= 60\n",
        "          start_time = time.time()\n",
        "          progress_bar = st.progress(0)\n",
        "          latest_iteration = st.empty()\n",
        "\n",
        "          for content_batch, _ in train_loader:\n",
        "            curr_batch_size = content_batch.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = TransformerNetwork(content_batch)\n",
        "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # Content Loss\n",
        "            MSELoss = nn.MSELoss().to(device)\n",
        "            content_loss = CONTENT_WEIGHT * MSELoss(content_features['relu2_2'], generated_features['relu2_2'])\n",
        "\n",
        "            # Style Loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = content_loss + style_loss\n",
        "\n",
        "            # Backprop and Weight Update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                torch.save(TransformerNetwork.state_dict(), f\"{DRIVE_PATH}/{MODEL_NAME}.pth\")\n",
        "            batch_count+=1\n",
        "            time_passed = time.time()-start_time\n",
        "            logs['TIME_PASSED'] = float(f\"{time_passed:.2f}\")\n",
        "\n",
        "            if 0 <= time_passed%600 <= 1:\n",
        "              if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "                with open(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\",'w') as f:\n",
        "                  json.dump(logs, f)\n",
        "            progress_time = time_passed/TRAIN_TIME\n",
        "            if progress_time < 1:\n",
        "              latest_iteration.text(f\"{(time_passed/60):.1f}/{TRAIN_TIME//60} mins\")\n",
        "              progress_bar.progress(progress_time)\n",
        "            else:\n",
        "              progress_bar.progress(1)\n",
        "\n",
        "            if (time.time()-start_time) > TRAIN_TIME:\n",
        "              logs['TRAING_COMPLETED'] = 1\n",
        "              if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "                with open(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\",'w') as f:\n",
        "                  json.dump(logs, f)\n",
        "              latest_iteration.success('Training completed!', icon=\"✅\")\n",
        "              break\n",
        "\n",
        "      else:\n",
        "        st.write(\"Generate and choose a style image\")\n",
        "\n",
        "    elif option == \"Continue with other networks training\":\n",
        "      tobedone = st.selectbox(\n",
        "      'Select the prompt to continue the training',\n",
        "      not_done)\n",
        "      PROMPT = tobedone\n",
        "      #TODO: load the pretrained weights, continue with network training, log the time passed and save the data to drive\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.jpg\"):\n",
        "        shutil.copy(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.jpg\",\"/content/style.jpg\")\n",
        "        if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "            with open(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\") as f:\n",
        "              logs = eval(f.read())\n",
        "\n",
        "        TRAIN_TIME = logs['TRAIN_TIME']\n",
        "\n",
        "        if st.button(\"Start training\"):\n",
        "          st.session_state['is_expanded_first'] = False\n",
        "          if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\"):\n",
        "            with st.spinner(\"Pretrained weights found. Loading it\"):\n",
        "              TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\", map_location=device))\n",
        "\n",
        "          VGG = VGG16('/content/vgg16.pth').to(device)\n",
        "          style_image = load_image(STYLE_IMAGE_PATH)\n",
        "          style_tensor = itot(style_image).to(device)\n",
        "          style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "          B, C, H, W = style_tensor.shape\n",
        "          style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "          style_gram = {}\n",
        "          for key, value in style_features.items():\n",
        "              style_gram[key] = gram(value)\n",
        "\n",
        "          # Optimizer settings\n",
        "          optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
        "          MODEL_NAME = \"style\"\n",
        "          DRIVE_PATH = f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"\n",
        "          batch_count = 1\n",
        "          start_time = logs['TIME_PASSED']\n",
        "          time_counter = time.time()\n",
        "          progress_bar = st.progress(logs['TIME_PASSED']/logs['TRAIN_TIME'])\n",
        "          latest_iteration = st.empty()\n",
        "\n",
        "          for content_batch, _ in train_loader:\n",
        "            curr_batch_size = content_batch.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = TransformerNetwork(content_batch)\n",
        "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # Content Loss\n",
        "            MSELoss = nn.MSELoss().to(device)\n",
        "            content_loss = CONTENT_WEIGHT * MSELoss(content_features['relu2_2'], generated_features['relu2_2'])\n",
        "\n",
        "            # Style Loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = content_loss + style_loss\n",
        "\n",
        "            # Backprop and Weight Update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                torch.save(TransformerNetwork.state_dict(), f\"{DRIVE_PATH}/{MODEL_NAME}.pth\")\n",
        "            batch_count+=1\n",
        "            time_passed = (time.time()-time_counter)+start_time\n",
        "            logs['TIME_PASSED'] = float(f\"{time_passed:.2f}\")\n",
        "\n",
        "            if 0 <= time_passed%300 <= 1:\n",
        "              if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "                with open(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\",'w') as f:\n",
        "                  json.dump(logs, f)\n",
        "            progress_time = time_passed/TRAIN_TIME\n",
        "            if progress_time < 1:\n",
        "              latest_iteration.text(f\"{(time_passed/60):.1f}/{TRAIN_TIME//60} mins\")\n",
        "              progress_bar.progress(progress_time)\n",
        "            else:\n",
        "              progress_bar.progress(1)\n",
        "\n",
        "            if time_passed > TRAIN_TIME:\n",
        "              logs['TRAING_COMPLETED'] = 1\n",
        "              if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\"):\n",
        "                with open(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/logs.json\",'w') as f:\n",
        "                  json.dump(logs, f)\n",
        "              latest_iteration.success('Training completed!', icon=\"✅\")\n",
        "              break\n",
        "\n",
        "INDEX = None\n",
        "solutions = {}\n",
        "\n",
        "with st.expander(\"Segment an Image\"):\n",
        "\n",
        "  segimgs = []\n",
        "  image_file = st.file_uploader(\"Upload an image to segment\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "  \n",
        "  if image_file is not None:\n",
        "    with open(\"content.jpg\",\"wb\") as f: \n",
        "      f.write(image_file.getbuffer())\n",
        "\n",
        "  if image_file is None:\n",
        "    if os.path.exists('content.jpg'):\n",
        "      os.remove('content.jpg')\n",
        "    if os.path.exists('utils/segment.jpg'):\n",
        "      os.remove(\"utils/segment.jpg\")\n",
        "      solutions = {}\n",
        "    if os.path.exists('segments'):\n",
        "      shutil.rmtree('segments')\n",
        "\n",
        "  if os.path.exists(\"content.jpg\") and (not os.path.exists(\"segments\")):\n",
        "    if st.button(\"Generate segments\"):\n",
        "      with st.spinner('Generating segments...'):\n",
        "        solutions = segments()\n",
        "        with open('utils/solutions.json', 'w') as f:\n",
        "          json.dump(json.dumps(solutions, cls=NumpyEncoder), f)\n",
        "\n",
        "  if os.path.exists(\"segments\") and len(segimgs)==0:\n",
        "    segimgs = [Image.open(\"segments/\"+f) for f in sorted(os.listdir(\"segments\"))]\n",
        "\n",
        "  if len(segimgs)>0:\n",
        "    INDEX = image_select(\n",
        "        label=\"Select a segment\",\n",
        "        images=segimgs,\n",
        "        return_value=\"index\",\n",
        "        use_container_width = False\n",
        "    )\n",
        "\n",
        "    if (INDEX or INDEX==0) and segimgs[INDEX]:\n",
        "      st.header(\"Selected Segment:\")\n",
        "      segimgs[INDEX].save('utils/segment.jpg')\n",
        "      st.image(segimgs[INDEX])\n",
        "\n",
        "with st.expander(\"Style transfer on the segment\"):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  if os.path.exists(\"utils/segment.jpg\") :\n",
        "    if st.button(\"Apply style transfer\"):\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "        content_image = load_image(\"utils/segment.jpg\")\n",
        "        content_tensor = itot(content_image).to(device)\n",
        "        TransformerNetwork = TransformerNetwork.to(device)\n",
        "        generated_tensor = TransformerNetwork(content_tensor)\n",
        "        generated_image = ttoi(generated_tensor.detach())\n",
        "        saveimg(generated_image, \"utils/segment_style.jpg\")\n",
        "\n",
        "      with st.spinner(\"Applying style transfer\"):\n",
        "        nimg = cv2.imread('content.jpg')[:, :, ::-1]\n",
        "        styleimg = cv2.imread('utils/segment_style.jpg')[:,:,::-1]\n",
        "        if os.path.exists('utils/solutions.json'):\n",
        "          with open('utils/solutions.json') as f:\n",
        "            solutions = json.loads(json.load(f))\n",
        "        \n",
        "        for i in solutions[str(INDEX)]:\n",
        "          nimg[i[0]][i[1]] = styleimg[i[0]][i[1]]\n",
        "\n",
        "        img = Image.fromarray(nimg.astype('uint8'))\n",
        "        img.save('Final Transformation.png')\n",
        "  else:\n",
        "    st.write(\"Upload a content image to generate segments\")\n",
        "\n",
        "if os.path.exists('Final Transformation.png'):\n",
        "\n",
        "  if 'final_trans' not in st.session_state:\n",
        "    st.session_state.final_trans = 'noimg'\n",
        "  st.image(\"Final Transformation.png\")\n",
        "  st.session_state.final_trans = 'yeimg'\n",
        "\n",
        "  if st.session_state.final_trans == 'yeimg':\n",
        "    try:\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"):\n",
        "        if st.button(\"Save content, style, transformed images to drive\"):\n",
        "          all_imgs =  [\n",
        "            'Final Transformation.png',\n",
        "            'content.jpg',\n",
        "            'style.jpg'\n",
        "        ]\n",
        "        for i in all_imgs:\n",
        "          os.system(f\"!cp {i} /content/drive/MyDrive/gen_imgs/{PROMPT}\")\n",
        "    except NameError:\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4EK0o6qRnn0"
      },
      "source": [
        "# Start the streamlit web app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!streamlit run --server.port 80 app.py&>logs.txt&\n",
        "from time import sleep\n",
        "sleep(5)\n",
        "from pyngrok import ngrok\n",
        "\n",
        "tunnels = ngrok.get_tunnels()\n",
        "if len(tunnels) > 0:\n",
        "  ngrok.kill()\n",
        "\n",
        "public_url = ngrok.connect(port='8501')\n",
        "print('Open the webapp at this URL:',public_url.data['public_url'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "207qafokLRzV",
        "outputId": "3bcd87b4-ca44-4887-cafd-67543a0de67a",
        "cellView": "form"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open the webapp at this URL: http://6e3c-34-124-230-34.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stop the streamlit webapp"
      ],
      "metadata": {
        "id": "-Xr6Vq9W19Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "ovOfFPAzMvMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BMkQbrriRj2t",
        "-Xr6Vq9W19Cz"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}