{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insaiyancvk/Mad-Artist/blob/main/streamlit_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdEmBazYY2_z"
      },
      "source": [
        "Note: connect to a GPU runtime, else it wouldn't work as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRHpHwwjRga9"
      },
      "source": [
        "# Setup \n",
        "&emsp;~ 8-20 mins"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title connect google drive\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "8gnMBo7hVtoD",
        "outputId": "43a4d440-2445-41b5-c3b1-ccc77e68abb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "VImiyFusNnbf"
      },
      "outputs": [],
      "source": [
        "#@title Stramlit Setup\n",
        "!pip install streamlit -q\n",
        "# !npm install localtunnel --quiet --loglevel=error\n",
        "!curl -sS -O https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken 2J28tpko02mtYUPnZiwzqrLmIwI_5WeUWyik4fHmXnqUe2UQ1\n",
        "!pip install -q pyngrok\n",
        "!curl -o /usr/local/lib/python3.8/dist-packages/google/protobuf/internal/builder.py https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python/google/protobuf/internal/builder.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gFlKG8ks12XM",
        "outputId": "6646b19e-74ac-44c3-fd9c-88186b02c800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 524 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 191 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 64.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 67.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 78.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61.5 MB 6.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Stable Diffusion Setup\n",
        "!pip -q install diffusers accelerate transformers scipy ftfy \"ipywidgets>=7,<8\"\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n",
        "!pip -q install streamlit streamlit-image-select\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"utils\"):\n",
        "  os.mkdir(\"utils\")\n",
        "\n",
        "if not os.path.exists(\"utils/robotoblack.ttf\"):\n",
        "  !curl -sS -o utils/robotoblack.ttf https://raw.githubusercontent.com/google/fonts/main/apache/roboto/static/Roboto-Black.ttf\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRbc4q9wQ-Xa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4d0bd7-beef-4cc6-bfda-2e26bb95b2b0",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train2014.zip       100%[===================>]  12.58G  64.0MB/s    in 4m 18s  \n",
            "/content/train\n",
            "/content\n",
            "vgg16-397923af.pth  100%[===================>] 527.79M   143MB/s    in 3.8s    \n"
          ]
        }
      ],
      "source": [
        "#@title Transformer network training utils setup\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/gen_imgs\"):\n",
        "  os.mkdir(\"/content/drive/MyDrive/gen_imgs\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# download and unzip dataset o train\n",
        "!wget -q http://images.cocodataset.org/zips/train2014.zip --show-progress\n",
        "!mkdir train\n",
        "%cd /content/train\n",
        "!unzip -qq /content/train2014.zip\n",
        "%cd ..\n",
        "\n",
        "!wget -q https://download.pytorch.org/models/vgg16-397923af.pth --show-progress\n",
        "!mv vgg16-397923af.pth vgg16.pth\n",
        "!rm train2014.zip\n",
        "SAVE_MODEL_PATH = \"/content/models/\"\n",
        "\n",
        "if not os.path.exists(SAVE_MODEL_PATH):\n",
        "  !mkdir $SAVE_MODEL_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywAaN8p7RCyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1f9163-0852-46e1-9e98-c659767c5644",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/utils\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#@title Segmentation related setup\n",
        "# Download helper scripts to generate mask data\n",
        "if not os.path.exists(\"utils\"):\n",
        "  %mkdir utils\n",
        "%cd utils\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/box_utils.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/config.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/output_utils.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/timer.py -q\n",
        "!wget https://raw.githubusercontent.com/insaiyancvk/yolact-mini/master/utils/yolact.py -q\n",
        "%cd ..\n",
        "!gdown -q \"1msxa5Gtl-SKfV9sG2ta5aVXRGzMct1uN&confirm=t\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMkQbrriRj2t"
      },
      "source": [
        "# The webapp script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ba-bCiSX5Fd",
        "outputId": "3de36f8f-9b01-4ca6-b86f-86c78cabe6ea",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dependencies.py\n"
          ]
        }
      ],
      "source": [
        "#@title Dependencies\n",
        "%%writefile dependencies.py\n",
        "from PIL import Image\n",
        "import os, cv2, torch, json\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, vgg_path=\"vgg16.pth\"):\n",
        "        super(VGG16, self).__init__()\n",
        "        # Load VGG Skeleton, Pretrained Weights\n",
        "        vgg16_features = models.vgg16(pretrained=False)\n",
        "        vgg16_features.load_state_dict(torch.load(vgg_path), strict=False)\n",
        "        self.features = vgg16_features.features\n",
        "\n",
        "        # Turn-off Gradient History\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers = {'3': 'relu1_2', '8': 'relu2_2', '15': 'relu3_3', '22': 'relu4_3'}\n",
        "        features = {}\n",
        "        for name, layer in self.features._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in layers:\n",
        "                features[layers[name]] = x\n",
        "                if (name=='22'):\n",
        "                    break\n",
        "\n",
        "        return features\n",
        "\n",
        "class TransformerNetworkNN(nn.Module):\n",
        "    \"\"\"Feedforward Transformation Network without Tanh\n",
        "    reference: https://arxiv.org/abs/1603.08155 \n",
        "    exact architecture: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerNetworkNN, self).__init__()\n",
        "        self.ConvBlock = nn.Sequential(\n",
        "            ConvLayer(3, 32, 9, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(64, 128, 3, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.ResidualBlock = nn.Sequential(\n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3), \n",
        "            ResidualLayer(128, 3)\n",
        "        )\n",
        "        self.DeconvBlock = nn.Sequential(\n",
        "            DeconvLayer(128, 64, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            DeconvLayer(64, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 3, 9, 1, norm=\"None\")\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvBlock(x)\n",
        "        x = self.ResidualBlock(x)\n",
        "        out = self.DeconvBlock(x)\n",
        "        return out\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        # Padding Layers\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm==\"instance\"):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm==\"batch\"):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        if (self.norm_type==\"None\"):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "class ResidualLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Residual Learning for Image Recognition\n",
        "\n",
        "    https://arxiv.org/abs/1512.03385\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=128, kernel_size=3):\n",
        "        super(ResidualLayer, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x                     # preserve residual\n",
        "        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n",
        "        out = self.conv2(out)            # 2nd conv layer\n",
        "        out = out + identity             # add residual\n",
        "        return out\n",
        "\n",
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "\n",
        "        # Transposed Convolution \n",
        "        padding_size = kernel_size // 2\n",
        "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm==\"instance\"):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm==\"batch\"):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_transpose(x)\n",
        "        if (self.norm_type==\"None\"):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "# Gram Matrix\n",
        "def gram(tensor):\n",
        "    B, C, H, W = tensor.shape\n",
        "    x = tensor.view(B, C, H*W)\n",
        "    x_t = x.transpose(1, 2)\n",
        "    return  torch.bmm(x, x_t) / (C*H*W)\n",
        "\n",
        "# Load image file\n",
        "def load_image(path):\n",
        "    # Images loaded as BGR\n",
        "    img = cv2.imread(path)\n",
        "    return img\n",
        "\n",
        "def saveimg(img, image_path):\n",
        "    img = img.clip(0, 255)\n",
        "    cv2.imwrite(image_path, img)\n",
        "\n",
        "# Preprocessing ~ Image to Tensor\n",
        "def itot(img, max_size=None):\n",
        "    # Rescale the image\n",
        "    if (max_size==None):\n",
        "        itot_t = transforms.Compose([\n",
        "            #transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])    \n",
        "    else:\n",
        "        H, W, C = img.shape\n",
        "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
        "        itot_t = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])\n",
        "\n",
        "    # Convert image to tensor\n",
        "    tensor = itot_t(img)\n",
        "\n",
        "    # Add the batch_size dimension\n",
        "    tensor = tensor.unsqueeze(dim=0)\n",
        "    return tensor\n",
        "\n",
        "# Preprocessing ~ Tensor to Image\n",
        "def ttoi(tensor):\n",
        "\n",
        "    # Remove the batch_size dimension\n",
        "    tensor = tensor.squeeze()\n",
        "    #img = ttoi_t(tensor)\n",
        "    img = tensor.cpu().numpy()\n",
        "    \n",
        "    # Transpose from [C, H, W] -> [H, W, C]\n",
        "    img = img.transpose(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "TRAIN_IMAGE_SIZE = 512\n",
        "DATASET_PATH = \"/content/train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"/content/style.jpg\"\n",
        "BATCH_SIZE = 4\n",
        "CONTENT_WEIGHT = 17\n",
        "STYLE_WEIGHT = 50\n",
        "TV_WEIGHT = 1e-6\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"/content/models/\"\n",
        "SAVE_IMAGE_PATH = \"/content/images/\"\n",
        "SAVE_MODEL_EVERY = 200 # 800 Images with batch size 4\n",
        "SEED = 68\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Dataset and Dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(TRAIN_IMAGE_SIZE),\n",
        "    transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.mul(255))\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Get Style Features\n",
        "imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float16).reshape(1,3,1,1).to(device)\n",
        "imagenet_mean = torch.tensor([103.939, 116.779, 123.68], dtype=torch.float16).reshape(1,3,1,1).to(device)\n",
        "\n",
        "## ----------------------------------------------     Segmentation stuff     ---------------------------------------------------------- ##\n",
        "from utils.yolact import Yolact\n",
        "from utils.output_utils import postprocess\n",
        "from utils.config import cfg\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def calc_size_preserve_ar(img_w, img_h, max_size):\n",
        "    ratio = sqrt(img_w / img_h)\n",
        "    w = max_size * ratio\n",
        "    h = max_size / ratio\n",
        "    return int(w), int(h)\n",
        "\n",
        "class FastBaseTransform(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.mean = torch.Tensor((103.94, 116.78, 123.68)).float().cuda()[None, :, None, None]\n",
        "          self.std  = torch.Tensor( (57.38, 57.12, 58.40) ).float().cuda()[None, :, None, None]\n",
        "    \n",
        "        else:\n",
        "          self.mean = torch.Tensor((103.94, 116.78, 123.68)).float()[None, :, None, None]\n",
        "          self.std  = torch.Tensor( (57.38, 57.12, 58.40) ).float()[None, :, None, None]\n",
        "    \n",
        "        self.transform = cfg.backbone.transform\n",
        "    \n",
        "    def forward(self, img):\n",
        "        self.mean = self.mean.to(img.device)\n",
        "        self.std  = self.std.to(img.device)\n",
        "        \n",
        "        # img assumed to be a pytorch BGR image with channel order [n, h, w, c]\n",
        "        if cfg.preserve_aspect_ratio:\n",
        "            _, h, w, _ = img.size()\n",
        "            img_size = calc_size_preserve_ar(w, h, cfg.max_size)\n",
        "            img_size = (img_size[1], img_size[0]) # Pytorch needs h, w\n",
        "        else:\n",
        "            img_size = (cfg.max_size, cfg.max_size)\n",
        "\n",
        "        img = img.permute(0, 3, 1, 2).contiguous()\n",
        "        img = F.interpolate(img, img_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        if self.transform.normalize:\n",
        "            img = (img - self.mean) / self.std\n",
        "        elif self.transform.subtract_means:\n",
        "            img = (img - self.mean)\n",
        "        elif self.transform.to_float:\n",
        "            img = img / 255\n",
        "        \n",
        "        if self.transform.channel_order != 'RGB':\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        img = img[:, (2, 1, 0), :, :].contiguous()\n",
        "\n",
        "        return img\n",
        "\n",
        "def evalimage(net:Yolact, path:str):\n",
        "    if torch.cuda.is_available():\n",
        "      frame = torch.from_numpy(cv2.imread(path)).cuda().float()\n",
        "    else:\n",
        "      frame = torch.from_numpy(cv2.imread(path)).float()\n",
        "    batch = FastBaseTransform()(frame.unsqueeze(0))\n",
        "    preds = net(batch)\n",
        "    h, w, _ = frame.shape\n",
        "    t = postprocess(preds, w, h, score_threshold = 0.15)\n",
        "    idx = t[1].argsort(0, descending=True)[:15]\n",
        "    masks = t[3][idx]\n",
        "    np.save('utils/mask_data', masks.cpu().numpy())\n",
        "\n",
        "def evaluate(net:Yolact, inp):\n",
        "    net.detect.use_fast_nms = True\n",
        "    net.detect.use_cross_class_nms = False\n",
        "    cfg.mask_proto_debug = False\n",
        "    evalimage(net, inp)\n",
        "    return\n",
        "\n",
        "\n",
        "def segments():\n",
        "    global solutions\n",
        "    with torch.no_grad():\n",
        "      if torch.cuda.is_available():\n",
        "          cudnn.fastest = True\n",
        "          torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "      else:\n",
        "          torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "      net = Yolact()\n",
        "      net.load_weights(\"yolact_base_54_800000.pth\")\n",
        "      net.eval()\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          net = net.cuda()\n",
        "\n",
        "      evaluate(net, \"content.jpg\")\n",
        "\n",
        "    mask = np.load('./utils/mask_data.npy')\n",
        "    sizes = {}\n",
        "    for x,i in enumerate(mask):\n",
        "        sizes[x] = len((np.argwhere(i!=0)))\n",
        "    sizes = {k: v for k, v in sorted(sizes.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "    solutions = {}\n",
        "    for x,i in enumerate(list(sizes.keys())[:5]):\n",
        "        solutions[x] = np.argwhere(mask[i] != 0)\n",
        "\n",
        "    img = cv2.imread('content.jpg')[:, :, ::-1]\n",
        "\n",
        "    segments = {}\n",
        "    for i in range(len(solutions)):\n",
        "        segments[i] = np.full((img.shape), 255.)\n",
        "\n",
        "    for x,i in solutions.items():\n",
        "        for j in i:\n",
        "            segments[x][j[0]][j[1]] = img[j[0]][j[1]]\n",
        "\n",
        "    if not os.path.exists('segments'):\n",
        "        os.mkdir('segments')\n",
        "\n",
        "    for i in range(len(segments)):\n",
        "        im = Image.fromarray(segments[i].astype('uint8'))\n",
        "        im.save(f'segments/{i}.jpg')\n",
        "    \n",
        "    return solutions\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjHQkbJ18RbG",
        "outputId": "77fbab30-e38c-4d7b-c6df-d895568018ee",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "#@title Streamlit web app\n",
        "%%writefile app.py\n",
        "import json\n",
        "import streamlit as st\n",
        "from streamlit_image_select import image_select\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from time import sleep\n",
        "from dependencies import \\\n",
        "                  TransformerNetworkNN, \\\n",
        "                  VGG16, \\\n",
        "                  device, \\\n",
        "                  load_image, \\\n",
        "                  itot, \\\n",
        "                  STYLE_IMAGE_PATH, \\\n",
        "                  imagenet_neg_mean, \\\n",
        "                  BATCH_SIZE, \\\n",
        "                  gram, \\\n",
        "                  optim, \\\n",
        "                  ADAM_LR, \\\n",
        "                  train_loader, \\\n",
        "                  CONTENT_WEIGHT, \\\n",
        "                  STYLE_WEIGHT, \\\n",
        "                  SAVE_MODEL_EVERY, \\\n",
        "                  NumpyEncoder, \\\n",
        "                  NUM_EPOCHS, \\\n",
        "                  segments, \\\n",
        "                  ttoi, \\\n",
        "                  saveimg\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch, os, glob, time, cv2, shutil\n",
        "from PIL import Image \n",
        "\n",
        "num_cols = 3\n",
        "num_rows = 3\n",
        "\n",
        "st.title(\"Text Based Style Transfer On Segmented Images\")\n",
        "\n",
        "if 'is_expanded_first' not in st.session_state:\n",
        "    st.session_state['is_expanded_first'] = True\n",
        "first_container = st.expander(\"Generate a Style Image\", expanded=st.session_state['is_expanded_first'])\n",
        "with first_container:\n",
        "  PROMPT = st.text_input(\"\",placeholder=\"Enter a prompt to generate images\")\n",
        "  st.write('Example: \"Japanese abstract art illustration\"')\n",
        "\n",
        "  gen_imgs = []\n",
        "\n",
        "  if not os.path.exists('Generated_images'):\n",
        "    os.mkdir('Generated_images')\n",
        "\n",
        "  if len(PROMPT) != 0:\n",
        "    if not os.path.exists(f\"Generated_images/{PROMPT}\"):\n",
        "      with st.info('Setting up Stable Diffusion Pipeline', icon=\"ℹ️\"):\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "          \"runwayml/stable-diffusion-v1-5\",\n",
        "          revision=\"fp16\",\n",
        "          torch_dtype=torch.float16,\n",
        "          low_cpu_mem_usage = True,\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "      prompt = [PROMPT] * num_cols # input from user\n",
        "\n",
        "      with st.spinner('Generating images...'):\n",
        "          for _ in range(num_rows):\n",
        "            images = pipe(prompt).images\n",
        "            gen_imgs.extend(images)\n",
        "\n",
        "      os.mkdir(f'Generated_images/{PROMPT}')\n",
        "\n",
        "      for x, i in enumerate(gen_imgs):\n",
        "        i.save(f'Generated_images/{PROMPT}/{x}.jpg')\n",
        "      \n",
        "      if not os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"):\n",
        "        os.mkdir(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\")\n",
        "      \n",
        "      for j,i in enumerate(gen_imgs):\n",
        "        i.save(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/{j}.jpg\")\n",
        "\n",
        "    if len(gen_imgs) == 0:\n",
        "      for f in glob.iglob(f\"Generated_images/{PROMPT}/*\"):\n",
        "          gen_imgs.append(Image.open(f))\n",
        "    all_images = gen_imgs\n",
        "\n",
        "    SELECTED_IMAGE = image_select(\n",
        "        label=\"Select an image\",\n",
        "        images=all_images,\n",
        "        captions=[str(i) for i in range(1,10)],\n",
        "        use_container_width = False\n",
        "    )\n",
        "\n",
        "    if SELECTED_IMAGE:\n",
        "      st.header(\"Selected Image:\")\n",
        "      st.image(SELECTED_IMAGE)\n",
        "    if st.button(\"Proceed further\"):\n",
        "      with st.spinner('Saving the style image'):\n",
        "        SELECTED_IMAGE.save('style.jpg')\n",
        "  sleep(2)\n",
        "  st.session_state['is_expanded_first'] = False\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/train\"):\n",
        "  TransformerNetwork = TransformerNetworkNN().to(device)\n",
        "  st.session_state['is_expanded_sec'] = False\n",
        "  if 'is_expanded_sec' not in st.session_state:\n",
        "    st.session_state['is_expanded_sec'] = True\n",
        "  sec_container = st.expander(\"Transformer network\", expanded=st.session_state['is_expanded_sec'])\n",
        "  with sec_container:\n",
        "    if os.path.exists(f'Generated_images/{PROMPT}'):\n",
        "      try:\n",
        "        del pipe\n",
        "        torch.cuda.empty_cache()\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "    option = st.selectbox(\n",
        "      'Select an option to proceed further',\n",
        "      ('Load a pretrained network', 'Train a transformer network'))\n",
        "\n",
        "    if option == \"Load a pretrained network\":\n",
        "      pretrained_prompt = st.text_input(\"\",placeholder=\"Enter the prompt was previously used for training the network\")\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/style.pth\"):\n",
        "        TRAIN_IMAGE_SIZE = 512\n",
        "        DATASET_PATH = \"/content/train\"\n",
        "        SEED = 68\n",
        "        device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        st.write(\"Loading the pretrained weights\")\n",
        "        TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{pretrained_prompt}/style.pth\", map_location=device))\n",
        "    \n",
        "    elif option == \"Train a transformer network\":\n",
        "      if os.path.exists(\"style.jpg\"):\n",
        "        \n",
        "        TRAIN_TIME = st.slider('Select a training time', 30, 120, 5)\n",
        "        st.write(\"Set training time (min). Higher the training time, better the style transfer by the network.\")\n",
        "        \n",
        "        if st.button(\"Start training\"):\n",
        "          st.session_state['is_expanded_first'] = False\n",
        "          if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\"):\n",
        "            with st.spinner(\"Pretrained weights found. Loading it\"):\n",
        "              TransformerNetwork.load_state_dict(torch.load(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}/style.pth\", map_location=device))\n",
        "          VGG = VGG16('/content/vgg16.pth').to(device)\n",
        "          style_image = load_image(STYLE_IMAGE_PATH)\n",
        "          style_tensor = itot(style_image).to(device)\n",
        "          style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "          B, C, H, W = style_tensor.shape\n",
        "          style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "          style_gram = {}\n",
        "          for key, value in style_features.items():\n",
        "              style_gram[key] = gram(value)\n",
        "\n",
        "          # Optimizer settings\n",
        "          optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
        "          MODEL_NAME = \"style\"\n",
        "          DRIVE_PATH = f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"\n",
        "          batch_count = 1\n",
        "          TRAIN_TIME *= 60\n",
        "          start_time = time.time()\n",
        "          progress_bar = st.progress(0)\n",
        "          latest_iteration = st.empty()\n",
        "\n",
        "          for content_batch, _ in train_loader:\n",
        "            curr_batch_size = content_batch.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = TransformerNetwork(content_batch)\n",
        "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # Content Loss\n",
        "            MSELoss = nn.MSELoss().to(device)\n",
        "            content_loss = CONTENT_WEIGHT * MSELoss(content_features['relu2_2'], generated_features['relu2_2'])\n",
        "\n",
        "            # Style Loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = content_loss + style_loss\n",
        "\n",
        "            # Backprop and Weight Update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                torch.save(TransformerNetwork.state_dict(), f\"{DRIVE_PATH}/{MODEL_NAME}.pth\")\n",
        "            batch_count+=1\n",
        "            time_passed = time.time()-start_time\n",
        "            progress_time = time_passed/TRAIN_TIME\n",
        "            if progress_time < 1:\n",
        "              latest_iteration.text(f\"{(time_passed/60):.1f}/{TRAIN_TIME//60} mins\")\n",
        "              progress_bar.progress(progress_time)\n",
        "            else:\n",
        "              progress_bar.progress(1)\n",
        "\n",
        "            if (time.time()-start_time) > TRAIN_TIME:\n",
        "              latest_iteration.success('Training completed!', icon=\"✅\")\n",
        "              break\n",
        "\n",
        "      else:\n",
        "        st.write(\"Generate and choose a style image\")\n",
        "\n",
        "INDEX = None\n",
        "solutions = {}\n",
        "\n",
        "with st.expander(\"Segment an Image\"):\n",
        "\n",
        "  segimgs = []\n",
        "  image_file = st.file_uploader(\"Upload an image to segment\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "  \n",
        "  if image_file is not None:\n",
        "    with open(\"content.jpg\",\"wb\") as f: \n",
        "      f.write(image_file.getbuffer())\n",
        "\n",
        "  if image_file is None:\n",
        "    if os.path.exists('content.jpg'):\n",
        "      os.remove('content.jpg')\n",
        "    if os.path.exists('utils/segment.jpg'):\n",
        "      os.remove(\"utils/segment.jpg\")\n",
        "      solutions = {}\n",
        "    if os.path.exists('segments'):\n",
        "      shutil.rmtree('segments')\n",
        "\n",
        "  if os.path.exists(\"content.jpg\") and (not os.path.exists(\"segments\")):\n",
        "    if st.button(\"Generate segments\"):\n",
        "      with st.spinner('Generating segments...'):\n",
        "        solutions = segments()\n",
        "        with open('utils/solutions.json', 'w') as f:\n",
        "          json.dump(json.dumps(solutions, cls=NumpyEncoder), f)\n",
        "\n",
        "  if os.path.exists(\"segments\") and len(segimgs)==0:\n",
        "    segimgs = [Image.open(\"segments/\"+f) for f in sorted(os.listdir(\"segments\"))]\n",
        "    \n",
        "    for f in os.listdir(\"segments\"):\n",
        "      segimgs.append(Image.open(\"segments/\"+f))\n",
        "\n",
        "  if len(segimgs)>0:\n",
        "    INDEX = image_select(\n",
        "        label=\"Select a segment\",\n",
        "        images=segimgs,\n",
        "        return_value=\"index\",\n",
        "        use_container_width = False\n",
        "    )\n",
        "\n",
        "    if (INDEX or INDEX==0) and segimgs[INDEX]:\n",
        "      st.header(\"Selected Segment:\")\n",
        "      segimgs[INDEX].save('utils/segment.jpg')\n",
        "      st.image(segimgs[INDEX])\n",
        "\n",
        "with st.expander(\"Style transfer on the segment\"):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  if os.path.exists(\"utils/segment.jpg\") :\n",
        "    if st.button(\"Apply style transfer\"):\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "        content_image = load_image(\"utils/segment.jpg\")\n",
        "        content_tensor = itot(content_image).to(device)\n",
        "        TransformerNetwork = TransformerNetwork.to(device)\n",
        "        generated_tensor = TransformerNetwork(content_tensor)\n",
        "        generated_image = ttoi(generated_tensor.detach())\n",
        "        saveimg(generated_image, \"utils/segment_style.jpg\")\n",
        "\n",
        "      with st.spinner(\"Applying style transfer\"):\n",
        "        nimg = cv2.imread('content.jpg')[:, :, ::-1]\n",
        "        styleimg = cv2.imread('utils/segment_style.jpg')[:,:,::-1]\n",
        "        if os.path.exists('utils/solutions.json'):\n",
        "          with open('utils/solutions.json') as f:\n",
        "            solutions = json.loads(json.load(f))\n",
        "        \n",
        "        for i in solutions[str(INDEX)]:\n",
        "          nimg[i[0]][i[1]] = styleimg[i[0]][i[1]]\n",
        "\n",
        "        img = Image.fromarray(nimg.astype('uint8'))\n",
        "        img.save('Final Transformation.png')\n",
        "  else:\n",
        "    st.write(\"Upload a content image to generate segments\")\n",
        "\n",
        "if os.path.exists('Final Transformation.png'):\n",
        "\n",
        "  if 'final_trans' not in st.session_state:\n",
        "    st.session_state.final_trans = 'noimg'\n",
        "  st.image(\"Final Transformation.png\")\n",
        "  st.session_state.final_trans = 'yeimg'\n",
        "\n",
        "  if st.session_state.final_trans == 'yeimg':\n",
        "    try:\n",
        "      if os.path.exists(f\"/content/drive/MyDrive/gen_imgs/{PROMPT}\"):\n",
        "        if st.button(\"Save content, style, transformed images to drive\"):\n",
        "          all_imgs =  [\n",
        "            'Final Transformation.png',\n",
        "            'content.jpg',\n",
        "            'style.jpg'\n",
        "        ]\n",
        "        for i in all_imgs:\n",
        "          os.system(f\"!cp {i} /content/drive/MyDrive/gen_imgs/{PROMPT}\")\n",
        "    except NameError:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4EK0o6qRnn0"
      },
      "source": [
        "# Start the streamlit web app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!streamlit run --server.port 80 app.py&>logs.txt&\n",
        "from time import sleep\n",
        "sleep(5)\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "207qafokLRzV",
        "outputId": "8169bebc-e9e2-44c9-a7ef-9475cc3b6937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://026f-34-138-246-89.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "ovOfFPAzMvMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PRHpHwwjRga9"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}